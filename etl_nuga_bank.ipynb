{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\admin\\onedrive\\documents\\10alytics_training\\pyspark\\nuga_project\\nuga_finance\\.venv\\lib\\site-packages (4.0.1)\n",
      "Requirement already satisfied: sqlalchemy in c:\\users\\admin\\onedrive\\documents\\10alytics_training\\pyspark\\nuga_project\\nuga_finance\\.venv\\lib\\site-packages (2.0.44)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\onedrive\\documents\\10alytics_training\\pyspark\\nuga_project\\nuga_finance\\.venv\\lib\\site-packages (2.3.3)\n",
      "Collecting psycopg2-binary\n",
      "  Using cached psycopg2_binary-2.9.11-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: py4j==0.10.9.9 in c:\\users\\admin\\onedrive\\documents\\10alytics_training\\pyspark\\nuga_project\\nuga_finance\\.venv\\lib\\site-packages (from pyspark) (0.10.9.9)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\admin\\onedrive\\documents\\10alytics_training\\pyspark\\nuga_project\\nuga_finance\\.venv\\lib\\site-packages (from sqlalchemy) (3.2.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\admin\\onedrive\\documents\\10alytics_training\\pyspark\\nuga_project\\nuga_finance\\.venv\\lib\\site-packages (from sqlalchemy) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\admin\\onedrive\\documents\\10alytics_training\\pyspark\\nuga_project\\nuga_finance\\.venv\\lib\\site-packages (from pandas) (2.3.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\onedrive\\documents\\10alytics_training\\pyspark\\nuga_project\\nuga_finance\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\onedrive\\documents\\10alytics_training\\pyspark\\nuga_project\\nuga_finance\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\onedrive\\documents\\10alytics_training\\pyspark\\nuga_project\\nuga_finance\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\onedrive\\documents\\10alytics_training\\pyspark\\nuga_project\\nuga_finance\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached psycopg2_binary-2.9.11-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "Installing collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.9.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark sqlalchemy pandas psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary dependencies\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise our Spark Session\n",
    "spark = SparkSession.builder.appName('NugaBankEtl').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.24:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>NugaBankEtl</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d572f8dbe0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Extraction\n",
    "nuga_bank_df = spark.read.csv(r'dataset\\nuga_bank_transactions.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+----------------+--------------+--------------------+------------------+--------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+------+--------------+\n",
      "|    Transaction_Date|Amount|Transaction_Type| Customer_Name|    Customer_Address|     Customer_City|Customer_State|    Customer_Country|             Company|           Job_Title|               Email|       Phone_Number|Credit_Card_Number|                IBAN|Currency_Code|Random_Number|Category|Group|Is_Active|        Last_Updated|         Description|Gender|Marital_Status|\n",
      "+--------------------+------+----------------+--------------+--------------------+------------------+--------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+------+--------------+\n",
      "|2024-03-23 15:38:...| 34.76|      Withdrawal|    James Neal|54912 Holmes Lodg...| West Keithborough|       Florida|                Togo|Benson, Johnson a...|                NULL|                NULL|  493.720.6609x7545|  3592901394693441|GB98RBPP090285271...|          MAD|       3167.0|       C|    Z|       No|2020-06-20 03:04:...|Yeah food anythin...| Other|      Divorced|\n",
      "|2024-04-22 19:15:...|163.92|      Withdrawal|   Thomas Long| 1133 Collin Passage|        Joshuabury|   Connecticut|Lao People's Demo...|                NULL|   Food technologist|michellelynch@exa...|      (497)554-3317|              NULL|GB03KFZR339662263...|          VEF|       2122.0|       B|    Z|     NULL|2020-12-27 13:23:...|Teach edge make n...|Female|       Married|\n",
      "|2024-04-12 19:46:...|386.32|      Withdrawal|Ashley Shelton|5297 Johnson Port...|       North Maria|    New Jersey|              Bhutan|       Jones-Mueller|Database administ...| ljordan@example.org|      (534)769-3072|      675983949974|GB59QYRN446730519...|          COP|       7796.0|       C|    Z|       No|2020-01-24 01:23:...|Again line face c...| Other|          NULL|\n",
      "|2024-04-17 15:29:...|407.15|         Deposit| James Rosario|56955 Moore Glens...|North Michellefurt|    New Mexico|             Iceland|       Vargas-Harris|Horticultural the...|parkerjames@examp...|+1-447-900-1320x257|     4761202519057|GB74FTDO268299438...|          BWP|       6284.0|       C|    Z|      Yes|2023-09-27 03:01:...|     Bag my a drive.|  NULL|          NULL|\n",
      "|2024-02-10 01:51:...|161.31|         Deposit|Miguel Leonard|262 Beck Expressw...|              NULL| West Virginia|             Eritrea|Richardson, Gonza...|   Minerals surveyor| zweaver@example.net|               NULL|   213156729655186|GB94EWRN587847592...|          SOS|       9179.0|       C|    Y|       No|2022-01-22 19:08:...|Husband find ok w...|Female|       Married|\n",
      "+--------------------+------+----------------+--------------+--------------------+------------------+--------------+--------------------+--------------------+--------------------+--------------------+-------------------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "nuga_bank_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Transaction_Date: timestamp (nullable = true)\n",
      " |-- Amount: double (nullable = true)\n",
      " |-- Transaction_Type: string (nullable = true)\n",
      " |-- Customer_Name: string (nullable = true)\n",
      " |-- Customer_Address: string (nullable = true)\n",
      " |-- Customer_City: string (nullable = true)\n",
      " |-- Customer_State: string (nullable = true)\n",
      " |-- Customer_Country: string (nullable = true)\n",
      " |-- Company: string (nullable = true)\n",
      " |-- Job_Title: string (nullable = true)\n",
      " |-- Email: string (nullable = true)\n",
      " |-- Phone_Number: string (nullable = true)\n",
      " |-- Credit_Card_Number: long (nullable = true)\n",
      " |-- IBAN: string (nullable = true)\n",
      " |-- Currency_Code: string (nullable = true)\n",
      " |-- Random_Number: double (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Group: string (nullable = true)\n",
      " |-- Is_Active: string (nullable = true)\n",
      " |-- Last_Updated: timestamp (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- Marital_Status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nuga_bank_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transaction_Date',\n",
       " 'Amount',\n",
       " 'Transaction_Type',\n",
       " 'Customer_Name',\n",
       " 'Customer_Address',\n",
       " 'Customer_City',\n",
       " 'Customer_State',\n",
       " 'Customer_Country',\n",
       " 'Company',\n",
       " 'Job_Title',\n",
       " 'Email',\n",
       " 'Phone_Number',\n",
       " 'Credit_Card_Number',\n",
       " 'IBAN',\n",
       " 'Currency_Code',\n",
       " 'Random_Number',\n",
       " 'Category',\n",
       " 'Group',\n",
       " 'Is_Active',\n",
       " 'Last_Updated',\n",
       " 'Description',\n",
       " 'Gender',\n",
       " 'Marital_Status']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Cleaning and Transformation\n",
    "\n",
    "nuga_bank_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no of rows\n",
    "num_rows = nuga_bank_df.count()\n",
    "\n",
    "num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no of columns \n",
    "num_columns = len(nuga_bank_df.columns)\n",
    "\n",
    "num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_Date Nulls 0\n",
      "Amount Nulls 0\n",
      "Transaction_Type Nulls 0\n",
      "Customer_Name Nulls 100425\n",
      "Customer_Address Nulls 100087\n",
      "Customer_City Nulls 100034\n",
      "Customer_State Nulls 100009\n",
      "Customer_Country Nulls 100672\n",
      "Company Nulls 100295\n",
      "Job_Title Nulls 99924\n",
      "Email Nulls 100043\n",
      "Phone_Number Nulls 100524\n",
      "Credit_Card_Number Nulls 100085\n",
      "IBAN Nulls 100300\n",
      "Currency_Code Nulls 99342\n",
      "Random_Number Nulls 99913\n",
      "Category Nulls 100332\n",
      "Group Nulls 100209\n",
      "Is_Active Nulls 100259\n",
      "Last_Updated Nulls 100321\n",
      "Description Nulls 100403\n",
      "Gender Nulls 99767\n",
      "Marital_Status Nulls 99904\n"
     ]
    }
   ],
   "source": [
    "# Checking for null values\n",
    "for column in nuga_bank_df.columns:\n",
    "    print(column, 'Nulls', nuga_bank_df.filter(nuga_bank_df[column].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to fill up missing values\n",
    "nuga_bank_df_clean = nuga_bank_df.fillna({\n",
    "    'Customer_Name' : 'Unknown',\n",
    "    'Customer_Address' : 'Unknown',\n",
    "    'Customer_City' : 'Unknown',\n",
    "    'Customer_State' : 'Unknown',\n",
    "    'Customer_Country' : 'Unknown',\n",
    "    'Company' : 'Unknown',\n",
    "    'Job_Title' : 'Unknown',\n",
    "    'Email' : 'Unknown',\n",
    "    'Phone_Number' : 'Unknown',\n",
    "    'Credit_Card_Number' : 0,\n",
    "    'IBAN' : 'Unknown',\n",
    "    'Currency_Code' : 'Unknown',\n",
    "    'Random_Number' : 0.0,\n",
    "    'Category' : 'Unknown',\n",
    "    'Group' : 'Unknown',\n",
    "    'Is_Active' : 'Unknown',\n",
    "    'Description' : 'Unknown',\n",
    "    'Gender' : 'Unknown',\n",
    "    'Marital_Status' : 'Unknown'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where last_updated is null\n",
    "nuga_bank_df_clean = nuga_bank_df_clean.na.drop(subset=['Last_Updated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction_Date Nulls 0\n",
      "Amount Nulls 0\n",
      "Transaction_Type Nulls 0\n",
      "Customer_Name Nulls 0\n",
      "Customer_Address Nulls 0\n",
      "Customer_City Nulls 0\n",
      "Customer_State Nulls 0\n",
      "Customer_Country Nulls 0\n",
      "Company Nulls 0\n",
      "Job_Title Nulls 0\n",
      "Email Nulls 0\n",
      "Phone_Number Nulls 0\n",
      "Credit_Card_Number Nulls 0\n",
      "IBAN Nulls 0\n",
      "Currency_Code Nulls 0\n",
      "Random_Number Nulls 0\n",
      "Category Nulls 0\n",
      "Group Nulls 0\n",
      "Is_Active Nulls 0\n",
      "Last_Updated Nulls 0\n",
      "Description Nulls 0\n",
      "Gender Nulls 0\n",
      "Marital_Status Nulls 0\n"
     ]
    }
   ],
   "source": [
    "# Checking for null values again\n",
    "for column in nuga_bank_df_clean.columns:\n",
    "    print(column, 'Nulls', nuga_bank_df_clean.filter(nuga_bank_df_clean[column].isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899679"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no of rows\n",
    "num_rows = nuga_bank_df_clean.count()\n",
    "\n",
    "num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------------+-------------+--------------------+-------------+--------------+----------------+-------------+------------------+-------------------+--------------------+--------------------+--------------------+-------------+-----------------+--------+-------+---------+--------------------+-------+--------------+\n",
      "|summary|            Amount|Transaction_Type|Customer_Name|    Customer_Address|Customer_City|Customer_State|Customer_Country|      Company|         Job_Title|              Email|        Phone_Number|  Credit_Card_Number|                IBAN|Currency_Code|    Random_Number|Category|  Group|Is_Active|         Description| Gender|Marital_Status|\n",
      "+-------+------------------+----------------+-------------+--------------------+-------------+--------------+----------------+-------------+------------------+-------------------+--------------------+--------------------+--------------------+-------------+-----------------+--------+-------+---------+--------------------+-------+--------------+\n",
      "|  count|            899679|          899679|       899679|              899679|       899679|        899679|          899679|       899679|            899679|             899679|              899679|              899679|              899679|       899679|           899679|  899679| 899679|   899679|              899679| 899679|        899679|\n",
      "|   mean|505.10367708927123|            NULL|         NULL|                NULL|         NULL|          NULL|            NULL|         NULL|              NULL|               NULL| 6.002851385999521E9|3.409662226750534E17|                NULL|         NULL|4952.920380491264|    NULL|   NULL|     NULL|                NULL|   NULL|          NULL|\n",
      "| stddev| 285.7945343300485|            NULL|         NULL|                NULL|         NULL|          NULL|            NULL|         NULL|              NULL|               NULL|2.3068371038619423E9|1.189655005733001E18|                NULL|         NULL|2966.543401151591|    NULL|   NULL|     NULL|                NULL|   NULL|          NULL|\n",
      "|    min|              10.0|         Deposit| Aaron Abbott|000 Aaron Landing...|    Aaronberg|       Alabama|     Afghanistan| Abbott Group|Academic librarian|            Unknown|       (200)201-4254|                   0|GB02AAAU191993009...|          AED|              0.0|       A|Unknown|       No|A American and to...| Female|      Divorced|\n",
      "|    max|            1000.0|      Withdrawal|    Zoe Young|             Unknown|  Zunigaville|       Wyoming|        Zimbabwe|Zuniga-Wilson|      Youth worker|zzuniga@example.org|             Unknown| 4999984361512569455|             Unknown|          ZWD|           9999.0| Unknown|      Z|      Yes|Yourself young ev...|Unknown|       Unknown|\n",
      "+-------+------------------+----------------+-------------+--------------------+-------------+--------------+----------------+-------------+------------------+-------------------+--------------------+--------------------+--------------------+-------------+-----------------+--------+-------+---------+--------------------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To view the summary statistics of the data\n",
    "nuga_bank_df_clean.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Transaction_Date',\n",
       " 'Amount',\n",
       " 'Transaction_Type',\n",
       " 'Customer_Name',\n",
       " 'Customer_Address',\n",
       " 'Customer_City',\n",
       " 'Customer_State',\n",
       " 'Customer_Country',\n",
       " 'Company',\n",
       " 'Job_Title',\n",
       " 'Email',\n",
       " 'Phone_Number',\n",
       " 'Credit_Card_Number',\n",
       " 'IBAN',\n",
       " 'Currency_Code',\n",
       " 'Random_Number',\n",
       " 'Category',\n",
       " 'Group',\n",
       " 'Is_Active',\n",
       " 'Last_Updated',\n",
       " 'Description',\n",
       " 'Gender',\n",
       " 'Marital_Status']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nuga_bank_df_clean.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transaction table\n",
    "transaction = nuga_bank_df_clean.select('Transaction_Date','Amount','Transaction_Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the transaction_id column\n",
    "transaction = transaction.withColumn('transaction_id', monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reordering the columns\n",
    "transaction = transaction.select('transaction_id', 'Transaction_Date','Amount','Transaction_Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+------+----------------+\n",
      "|transaction_id|    Transaction_Date|Amount|Transaction_Type|\n",
      "+--------------+--------------------+------+----------------+\n",
      "|             0|2024-03-23 15:38:...| 34.76|      Withdrawal|\n",
      "|             1|2024-04-22 19:15:...|163.92|      Withdrawal|\n",
      "|             2|2024-04-12 19:46:...|386.32|      Withdrawal|\n",
      "|             3|2024-04-17 15:29:...|407.15|         Deposit|\n",
      "|             4|2024-02-10 01:51:...|161.31|         Deposit|\n",
      "|             5|2024-02-10 22:56:...|764.34|        Transfer|\n",
      "|             6|2024-04-07 00:07:...|734.59|         Deposit|\n",
      "|             7|2024-03-08 01:51:...|592.43|         Deposit|\n",
      "|             8|2024-02-01 12:34:...| 927.1|         Deposit|\n",
      "|             9|2024-03-22 16:46:...| 66.59|        Transfer|\n",
      "|            10|2024-04-23 13:30:...| 246.3|      Withdrawal|\n",
      "|            11|2024-01-13 01:22:...|782.32|      Withdrawal|\n",
      "|            12|2024-02-25 15:16:...|818.42|      Withdrawal|\n",
      "|            13|2024-01-01 20:55:...|352.23|      Withdrawal|\n",
      "|            14|2024-01-19 00:01:...|316.19|      Withdrawal|\n",
      "|            15|2024-04-09 14:40:...|662.26|      Withdrawal|\n",
      "|            16|2024-04-15 04:58:...|893.73|         Deposit|\n",
      "|            17|2024-04-12 14:32:...|746.22|      Withdrawal|\n",
      "|            18|2024-02-26 14:43:...|214.77|      Withdrawal|\n",
      "|            19|2024-01-17 14:50:...|726.94|      Withdrawal|\n",
      "+--------------+--------------------+------+----------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "transaction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer table\n",
    "customer = nuga_bank_df_clean.select('Customer_Name', 'Customer_Address', 'Customer_City', 'Customer_State', \\\n",
    "                                    'Customer_Country','Email', 'Phone_Number').distinct()\n",
    "\n",
    "# add id column\n",
    "customer = customer.withColumn('customer_id', monotonically_increasing_id())\n",
    "\n",
    "# reorder the table\n",
    "customer = customer.select('customer_id', 'Customer_Name', 'Customer_Address', 'Customer_City', 'Customer_State', \\\n",
    "                                    'Customer_Country', 'Email', 'Phone_Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899679"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|customer_id|     Customer_Name|    Customer_Address|       Customer_City|Customer_State|    Customer_Country|               Email|        Phone_Number|\n",
      "+-----------+------------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "|          0|    Miguel Leonard|262 Beck Expressw...|             Unknown| West Virginia|             Eritrea| zweaver@example.net|             Unknown|\n",
      "|          1|           Unknown|             Unknown|         Evanchester|        Oregon|             Uruguay|             Unknown| (384)778-9942x91236|\n",
      "|          2|    Michael Murphy|894 Williams Ridg...|       Dominguezview|      New York|              Sweden|kristinstanley@ex...|+1-693-739-2204x8851|\n",
      "|          3|    Tina Gutierrez|    415 Taylor Knoll|           Donnastad|South Carolina|             Unknown|sarabrooks@exampl...|  623-933-0431x87174|\n",
      "|          4|      Kylie Adkins|    435 Nicole Curve|             Unknown|     Louisiana|             Unknown|davisronald@examp...|  (404)814-4457x1451|\n",
      "|          5|           Unknown|   36415 Chavez Oval|      Alejandroville|     Louisiana|                Peru|solomonheather@ex...|    894.974.9717x529|\n",
      "|          6|       Brian Glenn|505 Mcdowell Gard...|South Christinech...|  South Dakota|             Lesotho|bcabrera@example.net|001-962-928-1897x...|\n",
      "|          7|       Gary Archer| 0930 Michael Stream|            Boydport|North Carolina|Palestinian Terri...|kennethscott@exam...|       (713)965-2955|\n",
      "|          8|       Chase Smith|   156 Jeffrey Union|   Lake Nataliemouth|       Arizona|     North Macedonia|davidreeves@examp...|             Unknown|\n",
      "|          9|Mr. Gary Brooks II|   786 Mccarthy Fall|         Traceymouth|         Idaho|            Kiribati|ellislouis@exampl...|          8963308214|\n",
      "|         10|        Zoe Wilson|4575 Michael Harb...|        Alexanderton|        Alaska|           Venezuela|             Unknown|             Unknown|\n",
      "|         11|           Unknown|     0281 Diaz Locks|        Freemanburgh|        Oregon|Bouvet Island (Bo...|ygonzalez@example...|001-877-339-2451x...|\n",
      "|         12|           Unknown|4339 Hall Street ...|           Dianatown|       Unknown|             Moldova|robinsonbrian@exa...|             Unknown|\n",
      "|         13|    Zachary Grimes|42686 Jessica Loc...|           Johnshire|      Virginia|             Tunisia|vanessa93@example...|       (464)800-5827|\n",
      "|         14| Jessica Zimmerman|    15565 Reed Wells|       East Johnland|       Florida|                Chad|awilliams@example...|  664.840.4619x25293|\n",
      "|         15|     Tammy Simmons|997 Flores Locks ...|            Woodston|      Nebraska|                Oman| udecker@example.com|+1-853-249-6586x8571|\n",
      "|         16| Virginia Galloway|236 Harris Trail ...|       Lake Benjamin|      Michigan|United States of ...|   mross@example.com|   987.344.8192x3020|\n",
      "|         17|   Elizabeth Kline|  8389 Nicholson Run|           Jeffmouth|    Washington|             Liberia|matthewtaylor@exa...|    794-659-1946x128|\n",
      "|         18|     Matthew Costa|100 Crystal Gatew...|     Lake Leslieland|     Louisiana|         Timor-Leste|             Unknown|       (898)827-1111|\n",
      "|         19| Stephanie Simpson|             Unknown|          Murrayland|         Idaho|Northern Mariana ...|jacobgoodman@exam...|    592-707-2311x542|\n",
      "+-----------+------------------+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "customer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# employee table\n",
    "employees = nuga_bank_df_clean.select('Company', 'Job_Title', 'Gender', 'Marital_Status').distinct()\n",
    "\n",
    "# add id column\n",
    "employee = employees.withColumn('employee_id', monotonically_increasing_id())\n",
    "\n",
    "# re-order the dataframe\n",
    "employee = employee.select('employee_id', 'Company', 'Job_Title', 'Gender', 'Marital_Status')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-------+--------------+\n",
      "|employee_id|             Company|           Job_Title| Gender|Marital_Status|\n",
      "+-----------+--------------------+--------------------+-------+--------------+\n",
      "|          0|         Price Group|             Unknown|   Male|        Single|\n",
      "|          1|Rhodes, King and ...| Trade mark attorney|   Male|       Unknown|\n",
      "|          2|Schmidt, Morgan a...|     Engineer, water| Female|        Single|\n",
      "|          3|       Johnson Group|  Forensic scientist|   Male|       Unknown|\n",
      "|          4|     Phillips-Prince|Production assist...|Unknown|        Single|\n",
      "|          5|      Henry and Sons|Engineer, civil (...| Female|       Married|\n",
      "|          6|Thompson, Johnson...|Exercise physiolo...|  Other|       Unknown|\n",
      "|          7|Hernandez, Johnso...|Forensic psycholo...|Unknown|      Divorced|\n",
      "|          8|Carrillo, Schwart...| Solicitor, Scotland| Female|        Single|\n",
      "|          9|         Olson-Lucas| Magazine journalist|   Male|      Divorced|\n",
      "|         10|        Baxter-Knapp|Designer, televis...| Female|       Unknown|\n",
      "|         11|             Unknown|Programme researc...| Female|      Divorced|\n",
      "|         12|    Newton-Schneider|             Unknown|  Other|      Divorced|\n",
      "|         13|      Suarez-Terrell|            Best boy| Female|      Divorced|\n",
      "|         14|         Stewart LLC|       Archaeologist|   Male|        Single|\n",
      "|         15|Bernard, Sutton a...|Museum/gallery co...|  Other|       Unknown|\n",
      "|         16|Summers, Thornton...|Volunteer coordin...|Unknown|        Single|\n",
      "|         17|     Mcneil-Guerrero|     Ophthalmologist|Unknown|      Divorced|\n",
      "|         18|       Collins-Clark|Producer, televis...| Female|      Divorced|\n",
      "|         19|Lewis, Navarro an...|             Unknown|   Male|        Single|\n",
      "+-----------+--------------------+--------------------+-------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fact_table\n",
    "\n",
    "fact_table = nuga_bank_df_clean.join(customer, ['Customer_Name', 'Customer_Address', 'Customer_City', 'Customer_State', \\\n",
    "                                    'Customer_Country', 'Email', 'Phone_Number'], 'left') \\\n",
    "                                .join(transaction, ['Transaction_Date','Amount','Transaction_Type'], 'left') \\\n",
    "                                .join(employee, ['Company', 'Job_Title', 'Gender', 'Marital_Status'], 'left') \\\n",
    "                                .select('transaction_id', 'customer_id', 'employee_id', 'Credit_Card_Number', 'IBAN', 'Currency_Code', 'Random_Number', \\\n",
    "                                    'Category', 'Group', 'Is_Active', 'Last_Updated', 'Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------+-----------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+\n",
      "|transaction_id|customer_id|employee_id|Credit_Card_Number|                IBAN|Currency_Code|Random_Number|Category|Group|Is_Active|        Last_Updated|         Description|\n",
      "+--------------+-----------+-----------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+\n",
      "|   17179869198| 8590097714|     143861|   180067592769732|GB92JVMY004197871...|          EGP|       7198.0|       A|    Z|      Yes|2023-10-12 22:25:...|Before story prof...|\n",
      "|            18|25769812854|      41531|   213112163828334|GB50TJFN039979307...|          SVC|       7382.0|       B|    Z|      Yes|2020-01-19 18:19:...|Great evening so ...|\n",
      "|   25769803779|25769976749|     196153|                 0|GB32LGFL895760023...|          PAB|       8898.0|       A|    Y|      Yes|2021-12-07 15:35:...|Face field coach ...|\n",
      "|   17179869199| 8590087440|     140803|  4239162655922295|GB96MEEY268453596...|          BTN|       5605.0|       D|    Z|       No|2020-03-22 00:53:...|Visit present reg...|\n",
      "|    8589934596|25769892882|      68503|     4021800082481|GB78UBAH195883770...|          LBP|       8097.0|       C|    X|      Yes|2023-07-11 06:06:...|Maybe teacher dee...|\n",
      "|    8589934592|     105048|      58286|   213133896337542|GB07IUUE487965913...|          ISK|          0.0|       D|    Y|       No|2023-08-16 10:32:...|Themselves make ago.|\n",
      "|            20|25769846033|       1054|   213186454811670|GB77SPMZ984195063...|          SOS|       7610.0|       C|    X|       No|2021-04-15 13:30:...|Concern remember ...|\n",
      "|    8589934601|17179956876|      59273|  6011177559558424|GB76DNER497098023...|          BZD|       6499.0|       D|    X|  Unknown|2020-02-09 13:11:...|Risk take recent ...|\n",
      "|   25769803790| 8590158288|     174307|  6574717409681808|GB58ZWIH992989778...|          LSL|          0.0|       B|    Y|       No|2021-11-04 10:41:...|Any state recogni...|\n",
      "|   17179869184|     160577|     132736|    38737697224239|GB44WJZF814413539...|          JMD|       8569.0|       C|    Z|  Unknown|2021-01-25 12:59:...|             Unknown|\n",
      "|             8|25769811748|      29068|   180039947294310|GB48WNWB013807482...|          HRK|       3061.0|       B|    X|       No|2020-08-31 20:40:...|Notice paper son ...|\n",
      "|            11|25769849363|      38274|  4304956118653227|GB15VYWJ018711823...|          NIO|       2502.0|       B|    Y|      Yes|2021-04-12 20:23:...|Rest theory serve...|\n",
      "|    8589934605|     105049|      77576|   213123497507940|GB61RQDC156293863...|          GBP|          0.0|       B|    Z|      Yes|2023-01-29 06:25:...|    Plant very name.|\n",
      "|   25769803783|17180064840|     181965|    30472377458372|GB78IXJB107495171...|          MDL|       6463.0|       A|    Z|       No|2020-09-25 02:11:...|Country stage sam...|\n",
      "|   17179869197| 8590055338|     111370|      639006216746|GB81CQQR004909975...|      Unknown|       8199.0|       B|    Y|       No|2024-03-31 18:51:...|Try step sound. B...|\n",
      "|   17179869204| 8590094297|     116364|     4559268055614|             Unknown|          BBD|       5380.0|       D|    Y|       No|2024-04-16 14:15:...|Central improve q...|\n",
      "|   25769803789|     202061|      53257|  6011153251503935|GB28FTRX421798999...|          CHF|       1792.0|       C|    Z|      Yes|2020-07-14 19:25:...|Compare sell bank...|\n",
      "|             7|25769813966|      38273|     4500718397537|GB78FDAJ595830659...|          RON|       2445.0|       B|    X|       No|2022-11-19 08:59:...|             Unknown|\n",
      "|   25769803777|25770001535|      45126|   213172070656285|GB15ZCRW202730226...|          IRR|       3594.0|       D|    X|      Yes|2023-07-14 17:34:...|Reach minute with...|\n",
      "|            16|      31789| 8589985479|  3564854271916761|GB58VVHN678787830...|          AED|       2546.0|       B|    Y|      Yes|2021-09-14 07:15:...|Letter hand soldi...|\n",
      "+--------------+-----------+-----------+------------------+--------------------+-------------+-------------+--------+-----+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "fact_table.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o335.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:194)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# output the transformed data to parquet\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtransaction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdataset/transaction\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m customer.write.mode(\u001b[33m'\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m'\u001b[39m).parquet(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdataset/customer\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m employee.write.mode(\u001b[33m'\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m'\u001b[39m).parquet(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdataset/employee\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:2003\u001b[39m, in \u001b[36mDataFrameWriter.parquet\u001b[39m\u001b[34m(self, path, mode, partitionBy, compression)\u001b[39m\n\u001b[32m   2001\u001b[39m     \u001b[38;5;28mself\u001b[39m.partitionBy(partitionBy)\n\u001b[32m   2002\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(compression=compression)\n\u001b[32m-> \u001b[39m\u001b[32m2003\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o335.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:194)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "# output the transformed data to parquet\n",
    "#transaction.write.mode('overwrite').parquet(r'dataset/transaction')\n",
    "#customer.write.mode('overwrite').parquet(r'dataset/customer')\n",
    "#employee.write.mode('overwrite').parquet(r'dataset/employee')\n",
    "#fact_table.write.mode('overwrite').parquet(r'dataset/fact_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] No connection could be made because the target machine actively refused it",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mConnectionResetError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\py4j\\clientserver.py:527\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mConnectionResetError\u001b[39m: [WinError 10054] An existing connection was forcibly closed by the remote host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPy4JNetworkError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\py4j\\clientserver.py:530\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    529\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mError while sending or receiving.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[32m    531\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError while sending\u001b[39m\u001b[33m\"\u001b[39m, e, proto.ERROR_ON_SEND)\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mPy4JNetworkError\u001b[39m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# output the transformed data as csv\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mtransaction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.write.mode(\u001b[33m'\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m'\u001b[39m).option(\u001b[33m'\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m'\u001b[39m).csv(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdataset/transformeddata/csv/transaction\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m customer.repartition(\u001b[32m1\u001b[39m).write.mode(\u001b[33m'\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m'\u001b[39m).option(\u001b[33m'\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m'\u001b[39m).csv(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdataset/transformeddata/csv/customer\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m employee.repartition(\u001b[32m1\u001b[39m).write.mode(\u001b[33m'\u001b[39m\u001b[33moverwrite\u001b[39m\u001b[33m'\u001b[39m).option(\u001b[33m'\u001b[39m\u001b[33mheader\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m'\u001b[39m).csv(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdataset/transformeddata/csv/employee\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\pyspark\\sql\\classic\\dataframe.py:520\u001b[39m, in \u001b[36mDataFrame.repartition\u001b[39m\u001b[34m(self, numPartitions, *cols)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(numPartitions, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m    519\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(cols) == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumPartitions\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.sparkSession)\n\u001b[32m    521\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    522\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\n\u001b[32m    523\u001b[39m             \u001b[38;5;28mself\u001b[39m._jdf.repartition(numPartitions, \u001b[38;5;28mself\u001b[39m._jcols(*cols)),\n\u001b[32m    524\u001b[39m             \u001b[38;5;28mself\u001b[39m.sparkSession,\n\u001b[32m    525\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1057\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1055\u001b[39m         retry = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1056\u001b[39m     logging.info(\u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1059\u001b[39m     logging.exception(\n\u001b[32m   1060\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\py4j\\clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\py4j\\clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\OneDrive\\Documents\\10Alytics_Training\\pyspark\\Nuga_Project\\Nuga_Finance\\.venv\\Lib\\site-packages\\py4j\\clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [WinError 10061] No connection could be made because the target machine actively refused it"
     ]
    }
   ],
   "source": [
    "# output the transformed data as csv\n",
    "#transaction.repartition(1).write.mode('overwrite').option('header', 'true').csv(r'dataset/transformeddata/csv/transaction')\n",
    "#customer.repartition(1).write.mode('overwrite').option('header', 'true').csv(r'dataset/transformeddata/csv/customer')\n",
    "#employee.repartition(1).write.mode('overwrite').option('header', 'true').csv(r'dataset/transformeddata/csv/employee')\n",
    "#fact_table.repartition(1).write.mode('overwrite').option('header', 'true').csv(r'dataset/transformeddata/csv/fact_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert spark df to pandas df\n",
    "transaction_pd_df = transaction.toPandas()\n",
    "customer_pd_df = customer.toPandas()\n",
    "employee_pd_df = employee.toPandas()\n",
    "fact_table_pd_df = fact_table.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database, tables and data loaded successfully \n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset into a Postgresql DB\n",
    "\n",
    "# define database connection parameters\n",
    "db_params =  {\n",
    "    'username' : 'postgres',\n",
    "    'password' : 'London123',\n",
    "    'host' : 'localhost',\n",
    "    'port' : '5432',\n",
    "    'database' : 'nuga_bank'\n",
    "}\n",
    "\n",
    "# define the database connection url with db parameters\n",
    "db_url = f\"postgresql://{db_params['username']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['database']}\"\n",
    "\n",
    "# Create the database engine with the  db url\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "# Connect to PostgreSQL server\n",
    "with engine.connect() as connection:\n",
    "    # Create tables and load the data\n",
    "    transaction_pd_df.to_sql('transaction', connection, index=False, if_exists='replace')\n",
    "    customer_pd_df.to_sql('customer', connection, index=False, if_exists='replace')\n",
    "    employee_pd_df.to_sql('employee', connection, index=False, if_exists='replace')\n",
    "    fact_table_pd_df.to_sql('fact_table', connection, index=False, if_exists='replace')\n",
    "\n",
    "print('Database, tables and data loaded successfully ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
